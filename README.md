# Parser.py
В данном проекте используются следующие библиотеки:
```ruby
import requests
from bs4 import BeautifulSoup
import re
import json
import os
```
В качества доски объявлений был выбран сайт рекламного агентства "Частник-М".
На вход программе подаются две строки - название города и категория объявлений:
```ruby
Выберите название города: ['Все города', 'Абакан', 'Анжеро-Судженск', 'Белово', 'Березовский', 'Гурьевск', 'Калтан', 'Кемерово', 'Киселевск', 'Ленинск-Кузнецкий', 'Мариинск', 'Междуреченск', 'Мыски', 'Новокузнецк', 'Осинники', 'Полысаево', 'Прокопьевск', 'Салаир', 'Тайга', 'Таштагол', 'Топки', 'Юрга', 'Шерегеш', 'Тяжинский', 'Новосибирск']

Выберите название категории: ['все категории', 'все для дома', 'все для ремонта и строительства', 'детское', 'животные, растения', 'недвижимость', 'оборудование', 'одежда, обувь, аксессуары', 'отдых, спорт, туризм', 'работа', 'разное', 'транспорт', 'услуги', 'электроника, бытовая техника']
```
Затем парсер проверяет наличие объявлений по входным данным:
```ruby
soup = BeautifulSoup(src, "lxml")
p_check = soup.find("h3", class_="text-danger")
if p_check != None:
    print('Объявления не найдены')
    return -1
```
А также количество страниц объявлений, подходящих под условия, после чего сохраняет их в файлы формата HTML:
```ruby
try:
    page_count1= soup.find("ul", class_="pagination").find_all("a")[-2]
    page_count2 = page_count1.get('href')
    page_count = int(re.split("[=?&/]",page_count2)[5])
except AttributeError:
    page_count=1
for i in range(1, page_count + 1):
    nurl = url+f"&page={i}"
    r = requests.get(nurl, headers)
    with open(f"page_{i}.html", "w") as file:
        file.write(r.text)
```
Затем парсер просматривает каждое объявление и находит там данные, соответствующие следующим критериям:
```ruby
"Ссылка на изображение" : p_logo,
"Город" : p_city,
"Объявление" : p_name,
"Тип" : p_prof,
'Оплата' : p_earn,
'Телефон' : p_tel
```
На выходе пользователь получает JSON файл с данными о подходящих объявлениях: 
```

```
